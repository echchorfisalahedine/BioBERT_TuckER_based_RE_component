{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0UswAvgt3JZW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import json\n",
        "import csv\n",
        "import sys\n",
        "from collections import Counter\n",
        "import random\n",
        "from typing import List, Tuple\n",
        "\n",
        "# -------------------------\n",
        "# Device configuration\n",
        "# -------------------------\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ------------------------\n",
        "# Load Triples from CSV\n",
        "# ------------------------\n",
        "def load_triples_from_csv(csv_path: str,infer_path: str) -> List[Tuple[str, str, str]]:\n",
        "  triples = []\n",
        "  inference_triples = []\n",
        "  with open(csv_path, newline='\\n', encoding='utf-8') as csvfile:\n",
        "      reader = csv.reader(csvfile)\n",
        "      c = 0\n",
        "      for row in reader:\n",
        "          temp_row = row[0].split(';')\n",
        "          h, r, t = temp_row[0], temp_row[1], temp_row[2]\n",
        "          #if h in entity_list and t in entity_list and r in relation_list:\n",
        "            # if((h,r,t) in triples):\n",
        "            #   c = c+1\n",
        "            # else:\n",
        "          triples.append((h, r, t))\n",
        "      # print(c)\n",
        "\n",
        "  rel_list = [r for _,r,_ in triples]\n",
        "  rel_list = list(set(rel_list))\n",
        "  print(len(rel_list))\n",
        "  with open(infer_path, newline='\\n', encoding='utf-8') as csvfile:\n",
        "      reader = csv.reader(csvfile)\n",
        "      for row in reader:\n",
        "          temp_row = row[0].split(';')\n",
        "          h, r, t = temp_row[0], temp_row[1], temp_row[2]\n",
        "          #if h in entity_list and t in entity_list and r in relation_list:\n",
        "          if r in rel_list:\n",
        "            inference_triples.append((h, r, t))\n",
        "  return triples, inference_triples, rel_list\n",
        "\n",
        "def load_pairs_from_csv(csv_path: str) -> List[Tuple[str, str]]:\n",
        "  pairs = []\n",
        "  with open(csv_path, newline='\\n', encoding='utf-8') as csvfile:\n",
        "      reader = csv.reader(csvfile, delimiter='\\n')\n",
        "      c = 0\n",
        "      for row in reader:\n",
        "          temp_row = row[0].split(';')\n",
        "          h, t = temp_row[0], temp_row[1]\n",
        "          pairs.append((h, t))\n",
        "\n",
        "  return pairs\n",
        "\n",
        "def load_pairs_from_json(json_path: str) -> List[Tuple[str, str]]:\n",
        "  pairs = []\n",
        "  with open(json_path) as f:\n",
        "    pairs = json.load(f)\n",
        "\n",
        "  return pairs\n",
        "# -------------------------\n",
        "# Tucker scoring function\n",
        "# -------------------------\n",
        "def tucker_score(h, r, t, core_tensor):\n",
        "    return torch.einsum('bi,ijk,bj,bk->b', h, core_tensor, r, t)\n",
        "\n",
        "# -------------------------\n",
        "# Training step\n",
        "# -------------------------\n",
        "\n",
        "def train_softmax_core_tensor(\n",
        "    triples,\n",
        "    relation_vecs,        # dict[str, nn.Parameter]\n",
        "    relation_list,        # list[str]\n",
        "    core_tensor,          # nn.Parameter\n",
        "    tokenizer,\n",
        "    model,\n",
        "    optimizer,\n",
        "    scheduler,\n",
        "    num_epochs=10,\n",
        "    batch_size=32\n",
        "):\n",
        "    DEVICE = core_tensor.device\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0.0\n",
        "        random.shuffle(triples)\n",
        "\n",
        "        for i in range(0, len(triples), batch_size):\n",
        "            batch = triples[i:i + batch_size]\n",
        "\n",
        "            # --- Get head and tail BioBERT vectors ---\n",
        "            h_vecs = torch.stack([get_biobert_vector(h, tokenizer, model) for h, _, _ in batch]).to(DEVICE)\n",
        "            t_vecs = torch.stack([get_biobert_vector(t, tokenizer, model) for _, _, t in batch]).to(DEVICE)\n",
        "\n",
        "            # --- Prepare relation embeddings ---\n",
        "            all_rel_vecs = torch.stack([relation_vecs[r] for r in relation_list]).to(DEVICE)  # [R, D]\n",
        "\n",
        "            all_scores = []\n",
        "            targets = []\n",
        "\n",
        "            for j, (h, r, t) in enumerate(batch):\n",
        "                h_i = h_vecs[j].unsqueeze(0).expand(len(relation_list), -1)  # [R, D]\n",
        "                t_i = t_vecs[j].unsqueeze(0).expand(len(relation_list), -1)  # [R, D]\n",
        "                r_i = all_rel_vecs\n",
        "\n",
        "                scores_i = tucker_score(h_i, r_i, t_i, core_tensor)  # [R]\n",
        "                all_scores.append(scores_i)\n",
        "\n",
        "                target_index = relation_list.index(r)\n",
        "                targets.append(target_index)\n",
        "\n",
        "            all_scores = torch.stack(all_scores)  # [B, R]\n",
        "            targets = torch.tensor(targets).to(DEVICE)\n",
        "\n",
        "            loss = loss_fn(all_scores, targets)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {total_loss:.4f}\")\n",
        "        scheduler.step(total_loss)\n",
        "        # for param_group in optimizer.param_groups:\n",
        "        #   print(f\"Current learning rate: {param_group['lr']}\")\n",
        "\n",
        "\n",
        "def get_biobert_vector(label, tokenizer, model):\n",
        "    inputs = tokenizer(label, return_tensors=\"pt\", padding=True, truncation=True).to(DEVICE)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    # Mean pooling\n",
        "    mask = inputs['attention_mask'].unsqueeze(-1).expand(outputs.last_hidden_state.size()).float()\n",
        "    summed = torch.sum(outputs.last_hidden_state * mask, dim=1)\n",
        "    counts = torch.clamp(mask.sum(1), min=1e-9)\n",
        "    return (summed / counts).squeeze(0)\n",
        "\n",
        "\n",
        "# ------------------------\n",
        "# Inference Function\n",
        "# ------------------------\n",
        "def predict_relation_from_biobert(\n",
        "    h_label,\n",
        "    t_label,\n",
        "    relation_list,\n",
        "    relation_vecs,\n",
        "    core_tensor,\n",
        "    tokenizer,\n",
        "    biobert_model,\n",
        "    device\n",
        "):\n",
        "    # Encode head and tail using BioBERT\n",
        "    h_vec = get_biobert_vector(h_label, tokenizer, biobert_model).to(device)\n",
        "    t_vec = get_biobert_vector(t_label, tokenizer, biobert_model).to(device)\n",
        "\n",
        "    # Precompute once\n",
        "    h_vec = h_vec.unsqueeze(0)\n",
        "    t_vec = t_vec.unsqueeze(0)\n",
        "\n",
        "    scores = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for rel in relation_list:\n",
        "            r_vec = relation_vecs[rel].to(device).unsqueeze(0)\n",
        "            score = tucker_score(h_vec, r_vec, t_vec, core_tensor.to(device)).item()\n",
        "            scores.append((rel, score))\n",
        "\n",
        "    return sorted(scores, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "\n",
        "def evaluate_predictions(inference_triples, core_tensor, relation_list, relation_vecs):\n",
        "    \"\"\"\n",
        "    Evaluates the model performance using Hits@K and MRR metrics.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary of evaluation metrics.\n",
        "    \"\"\"\n",
        "    hits_at_1 = 0\n",
        "    hits_at_3 = 0\n",
        "    hits_at_10 = 0\n",
        "    ranks = []\n",
        "    preds = []\n",
        "\n",
        "    for h, r, t in inference_triples:\n",
        "        predictions = predict_relation_from_biobert(h, t, relation_list, relation_vecs, core_tensor, tokenizer, model, DEVICE)\n",
        "        preds.append(predictions)\n",
        "        predicted_labels = [pred[0] for pred in predictions]\n",
        "\n",
        "        try:\n",
        "            rank = predicted_labels.index(r) + 1  # 1-based index\n",
        "        except ValueError:\n",
        "            rank = len(predicted_labels) + 1  # if not found\n",
        "\n",
        "        ranks.append(rank)\n",
        "        if rank == 1:\n",
        "            hits_at_1 += 1\n",
        "        if rank <= 3:\n",
        "            hits_at_3 += 1\n",
        "        if rank <= 10:\n",
        "            hits_at_10 += 1\n",
        "\n",
        "    total = len(inference_triples)\n",
        "    mrr = sum(1.0 / rank for rank in ranks) / total\n",
        "    mean_rank = sum(ranks) / total\n",
        "\n",
        "    return preds,{\n",
        "        \"Hits@1\": hits_at_1 / total,\n",
        "        \"Hits@3\": hits_at_3 / total,\n",
        "        \"Hits@10\": hits_at_10 / total,\n",
        "        \"MRR\": mrr,\n",
        "        \"Mean Rank\": mean_rank,\n",
        "        \"Total Samples\": total\n",
        "    }\n",
        "\n",
        "\n",
        "def infer_model(inference_triples, core_tensor, relation_list, relation_vecs):\n",
        "    \"\"\"\n",
        "    Evaluates the model performance using Hits@K and MRR metrics.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary of evaluation metrics.\n",
        "    \"\"\"\n",
        "\n",
        "    preds = []\n",
        "    final_pairs = []\n",
        "\n",
        "    for tr in inference_triples:\n",
        "        h = tr[\"head\"]\n",
        "        t = tr[\"tail\"]\n",
        "        predictions = predict_relation_from_biobert(h[\"mondo_label\"], t[\"mondo_label\"], relation_list, relation_vecs, core_tensor, tokenizer, model, DEVICE)\n",
        "        if(predictions[0][0] == 'part of' or predictions[0][0] == 'has part'):\n",
        "          if(h[\"label\"] == t[\"label\"] and t[\"label\"] != \"B-Disease\"):\n",
        "              preds.append(predictions)\n",
        "              final_pairs.append((h,t))\n",
        "        elif(predictions[0][0] == 'disease has location'):\n",
        "          if(h[\"label\"] == \"B-Disease\" and t[\"label\"] == \"B-Cell\"):\n",
        "            preds.append(predictions)\n",
        "            final_pairs.append((h,t))\n",
        "        elif(predictions[0][0] == 'disease has feature'):\n",
        "          if(h[\"label\"] == \"B-Disease\" and t[\"label\"] == \"B-Disease\"):\n",
        "            preds.append(predictions)\n",
        "            final_pairs.append((h,t))\n",
        "        elif(predictions[0][0] == 'disease has infectious agent'):\n",
        "          if(h[\"label\"] == \"B-Disease\" and t[\"label\"] == \"B-Disease\"):\n",
        "            preds.append(predictions)\n",
        "            final_pairs.append((h,t))\n",
        "        elif(predictions[0][0] == 'disease caused by disruption'):\n",
        "          if(h[\"label\"] == \"B-Disease\" and t[\"label\"] == \"B-Gene_or_gene_product\"):\n",
        "            preds.append(predictions)\n",
        "            final_pairs.append((h,t))\n",
        "        elif(predictions[0][0] == 'develops from'):\n",
        "          if(h[\"label\"] == \"B-Disease\" and t[\"label\"] == \"B-Disease\"):\n",
        "            preds.append(predictions)\n",
        "            final_pairs.append((h,t))\n",
        "        elif(predictions[0][0] == 'has material basis in germline mutation in'):\n",
        "          if(h[\"label\"] == \"B-Disease\" and t[\"label\"] == \"B-Gene_or_gene_product\"):\n",
        "            preds.append(predictions)\n",
        "            final_pairs.append((h,t))\n",
        "        elif(predictions[0][0] == 'is a'):\n",
        "          if(h[\"label\"] == t[\"label\"]):\n",
        "            preds.append(predictions)\n",
        "            final_pairs.append((h,t))\n",
        "\n",
        "    print(len(final_pairs))\n",
        "    return final_pairs, preds\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "def evaluate_predictions_with_f1(inference_triples, core_tensor, relation_list, relation_vecs):\n",
        "    hits_at_1 = 0\n",
        "    hits_at_3 = 0\n",
        "    hits_at_10 = 0\n",
        "    ranks = []\n",
        "    preds = []\n",
        "\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    for h, r, t in inference_triples:\n",
        "        predictions = predict_relation_from_biobert(\n",
        "            h, t, relation_list, relation_vecs, core_tensor, tokenizer, model, DEVICE\n",
        "        )\n",
        "        preds.append(predictions)\n",
        "        predicted_labels = [pred[0] for pred in predictions]\n",
        "\n",
        "        # Append labels for classification\n",
        "        y_true.append(r)\n",
        "        y_pred.append(predicted_labels[0])  # top-1 prediction\n",
        "\n",
        "        try:\n",
        "            rank = predicted_labels.index(r) + 1\n",
        "        except ValueError:\n",
        "            rank = len(predicted_labels) + 1\n",
        "\n",
        "        ranks.append(rank)\n",
        "        if rank == 1:\n",
        "            hits_at_1 += 1\n",
        "        if rank <= 3:\n",
        "            hits_at_3 += 1\n",
        "        if rank <= 10:\n",
        "            hits_at_10 += 1\n",
        "\n",
        "    total = len(inference_triples)\n",
        "    mrr = sum(1.0 / rank for rank in ranks) / total\n",
        "    mean_rank = sum(ranks) / total\n",
        "\n",
        "    # Calculate classification metrics\n",
        "    precision = precision_score(y_true, y_pred, average='micro', zero_division=0)\n",
        "    recall = recall_score(y_true, y_pred, average='micro', zero_division=0)\n",
        "    f1 = f1_score(y_true, y_pred, average='micro', zero_division=0)\n",
        "\n",
        "    return preds,{\n",
        "        \"Hits@1\": hits_at_1 / total,\n",
        "        \"Hits@3\": hits_at_3 / total,\n",
        "        \"Hits@10\": hits_at_10 / total,\n",
        "        \"MRR\": mrr,\n",
        "        \"Mean Rank\": mean_rank,\n",
        "        \"Precision\": precision,\n",
        "        \"Recall\": recall,\n",
        "        \"F1\": f1,\n",
        "        \"Total Samples\": total\n",
        "    }\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Example usage\n",
        "# -------------------------\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    BERT_MODEL_NAME = 'dmis-lab/biobert-base-cased-v1.1'\n",
        "    tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL_NAME)\n",
        "    model = AutoModel.from_pretrained(BERT_MODEL_NAME).to(DEVICE)\n",
        "\n",
        "    triples, inference_triples, rel_list = load_triples_from_csv(\"triples_mondo_10_relations.csv\",\"triples_mondo_10_relations_test.csv\")\n",
        "    print(f\"Loaded {len(triples)} valid triples.\")\n",
        "\n",
        "\n",
        "    # Learnable core tensor\n",
        "    core_tensor = nn.Parameter(torch.randn(768, 768, 768))\n",
        "    relation_vecs = {r: torch.nn.Parameter(torch.randn(768)) for r in rel_list}\n",
        "    # OR\n",
        "    # # Load core tensor\n",
        "    # core_data = torch.load(\"drive/MyDrive/tucker_files/core_tensor_bio.pt\", map_location='cpu')\n",
        "    # core_tensor = core_data['core_tensor']\n",
        "\n",
        "    # # Load relation vectors\n",
        "    # rel_data = torch.load(\"drive/MyDrive/tucker_files/relation_vecs_bio.pt\", map_location='cpu')\n",
        "    # relation_vecs = rel_data['relation_vecs']\n",
        "\n",
        "    # Optimizer and loss\n",
        "    core_tensor_optimizer  = torch.optim.Adam([\n",
        "          {'params': [core_tensor]},\n",
        "          {'params': list(relation_vecs.values())}\n",
        "      ], lr=1e-3)\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(core_tensor_optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
        "\n",
        "    # loss_fn = nn.MSELoss()\n",
        "    loss_fn = nn.MarginRankingLoss(margin=1.0)\n",
        "\n",
        "    # Train\n",
        "    train_softmax_core_tensor(\n",
        "        triples,\n",
        "        relation_vecs,        # dict[str, nn.Parameter]\n",
        "        rel_list,        # list[str]\n",
        "        core_tensor,          # nn.Parameter\n",
        "        tokenizer,\n",
        "        model,\n",
        "        core_tensor_optimizer,\n",
        "        scheduler,\n",
        "        num_epochs=30,\n",
        "        batch_size=32\n",
        "    )\n",
        "\n",
        "    torch.save({'core_tensor': core_tensor}, \"core_tensor_bio.pt\")\n",
        "    torch.save({'relation_vecs': relation_vecs}, \"relation_vecs_bio.pt\")\n",
        "    from google.colab import files\n",
        "    files.download('core_tensor_bio.pt')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    file1= open('results.txt','w')\n",
        "\n",
        "    print(\"\\n=== Sample Inference ===\")\n",
        "    predictions, metrics = evaluate_predictions_with_f1(inference_triples, core_tensor, rel_list, relation_vecs)\n",
        "    print(\"\\n=== Evaluation Metrics ===\")\n",
        "    for k, v in metrics.items():\n",
        "      print(f\"{k}: {v:.4f}\" if isinstance(v, float) else f\"{k}: {v}\")\n",
        "      file1.write(f\"{k}: {v:.4f}\" if isinstance(v, float) else f\"{k}: {v}\")\n",
        "      file1.write('\\n')\n",
        "\n",
        "    print(\"\\n=== Sample Inference ===\")\n",
        "    for i in range(len(inference_triples)):\n",
        "        h, r, t = inference_triples[i]\n",
        "        print(f\"\\nHead: {h} | Tail: {t}\")\n",
        "        print(f\"True: {r}\")\n",
        "        print(\"Top predictions:\")\n",
        "        file1.write(f\"\\nHead: {h} | Tail: {t}\")\n",
        "        file1.write('\\n')\n",
        "        file1.write(f\"True: {r}\")\n",
        "        file1.write('\\n')\n",
        "        file1.write(\"Top predictions:\")\n",
        "        file1.write('\\n')\n",
        "        for rel, score in predictions[i]:\n",
        "            print(f\"  {rel:25} Score: {score:.4f}\")\n",
        "            file1.write(f\"  {rel:25} Score: {score:.4f}\")\n",
        "            file1.write('\\n')"
      ]
    }
  ]
}